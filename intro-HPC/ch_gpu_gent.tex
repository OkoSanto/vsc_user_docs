\chapter{HPC-UGent GPGPU cluster}
\label{ch:gpu_ugent}


\section{Submitting jobs}
\label{sec:gpu_ugent_jobs}

To submit jobs to the HPC-UGent GPU cluster nicknamed \lstinline|joltik|, first use:

\begin{prompt}
%\shellcmd{module swap cluster/joltik}
\end{prompt}

Then use the familiar \lstinline|qsub|, \lstinline|qstat|, etc.\ commands, taking into account the guidelines outlined
in section~\ref{sec:gpu_ugent_resources}.

\subsection{Interactive jobs}
\label{sec:gpu_ugent_interactive_jobs}

To interactively experiment with GPUs, you can submit an interactive job using \lstinline|qsub -I| (and request
one or more GPUs, see section~\ref{sec:gpu_ugent_resources}).

Note that due to a bug in Slurm you will currently not be able to be able to interactively use
MPI software that requires access to the GPUs. If you need this, please contact use via \hpcinfo.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Hardware}

See \url{https://www.ugent.be/hpc/en/infrastructure/overzicht.htm}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Requesting (GPU) resources %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Requesting (GPU) resources}
\label{sec:gpu_ugent_resources}

There are 2 main ways to ask for GPUs as part of a job:

\begin{itemize}
  \item Either as a node property (similar to the number of cores
  per node specified via \lstinline|ppn|) using \lstinline|-l nodes=X:ppn=Y:gpus=Z| (where the \lstinline|ppn=Y| is optional),
  or as a separate resource request (similar to the amount of memory) via \lstinline|-l gpus=Z|.
  Both notations give exactly the same result.
  The \lstinline|-l gpus=Z| is convenient is you only need one
  node and you are fine with the default number of cores per GPU.
  The \lstinline|-l nodes=...:gpus=Z| notation is required if you want to run with full control or in multinode cases like MPI jobs.
  If you do not specify the number of GPUs by just using \lstinline|-l gpus|, you get by default 1 GPU.
\item As a resource of it's own, via \lstinline|--gpus X|.
  In this case however, you are \emph{not} guaranteed that the GPUs are on the same node,
  so your script or code must be able to deal with this.
%  TODO We are providing a parallel wrapper ``wurker'' in the ``vsc-mympirun'' module to help with this
%  (and with other more usual parallel work, similar to the usual ``worker'' or ``atools'' tools).
%\item As a partial node resource, via ``-l nodes=...:mps=Z'' or ``-l mps=Z''.
%  This triggers the Multi-Process Service (MPS, see https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf),
%  a way to ask for part of a GPU. The ``mps=`` value is a percentage of a GPU, and when submitting the job,
%  you typically ask for a multiple of 100. The jobscript can then hand out portions of this (e.g. 50 per task) to the actual work.
%  This is useful when a single application or MPI task cannot utilise a single/full GPU, and there are many other similar tasks that
%  need to be processed or increasing the MPI ranks gives a speedup (e.g. when there is a significant portion of CPU work in the code).
%  Unfortunately, this is not a silver bullet, and might require some experimenting to found out any potential benefits and proper tuning.
%  TODO: how can a user now that an application is not using the full gpu resources?
%  TODO this needs testing and there are some constraints (eg one mps job per node and thus one user per node using MPS)
%  TODO needs proper integration with mypmirun / wurker
%  TODO add separate section on MPS
\end{itemize}

Some background:
\begin{itemize}
\item The GPUs are constrained to the jobs (like the CPU cores), but do not run in so-called ``exclusive'' mode.
\item The GPUs run with the so-called ``persistence daemon'', so the GPUs is not re-initialised between jobs.
%  TODO: we need to fix this in the pro/epilogue scripts? is this similar to regular RAM?
\end{itemize}


%TODO: add examples ?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%
% Attention points %
%%%%%%%%%%%%%%%%%%%%

\section{Attention points}
\label{sec:gpu_ugent_attention_points}

Some important attention points:

\begin{itemize}
\item For MPI jobs, we recommend the (new) wrapper \lstinline|mypmirun| from the \lstinline|vsc-mympirun| module
  (\lstinline|pmi| is the background mechanism to start the MPI tasks,
  and is different from the usual \lstinline|mpirun| that is used by the \lstinline|mympirun| wrapper).
  At some later point, we \emph{might} promote the \lstinline|mypmirun| tool or rename it,
  to avoid the confusion in the naming).
\item Sharing GPUs requires MPS. The Slurm built-in MPS does not really do want you want, so we will provide integration with
  \lstinline|mypmirun| and \lstinline|wurker|.
\item For parallel work, we are working on a \lstinline|wurker| wrapper from the \lstinline|vsc-mympirun| module that supports
  GPU placement and MPS, without any limitations wrt the requested resources (i.e. also support the case where GPUs
  are spread heterogenous over nodes from using the \lstinline|--gpus Z| option).
\item Both \lstinline|mypmirun| and \lstinline|wurker| will try to do the most optimised placement of cores and tasks, and
  will provide 1 (optimal) GPU per task/MPI rank, and set one so-called \emph{visible device}
  (i.e. \lstinline|CUDA_VISIBLE_DEVICES| only has 1 ID). The actual devices are not constrained to the ranks,
  so you can access all devices requested in the job.
  \emph{We know that at this moment, this is not working properly, but we are working on this. We advise against trying to fix this yourself.}
  % TODO: this is still not the case, due to bugs in slurm. For now, you will probably do not get optimal placement and/or more
  %than one visible device.
  %TODO: we need an easy way to toggle this behaviour.
  %TODO: should be configurable from qsub somehow. Or we need to wait for fix or patch it ourself.
\end{itemize}

%TODO: add section on mypmirun, but has nothing to do with joltik. It works on all ugent/slurm clusters with and supports intelmpi.
%Main advantages are single tool instead of per-MPI mpirun flavour, improved accouting and faster startup.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%
% Software %
%%%%%%%%%%%%

\section{Software with GPU support}
\label{sec:gpu_ugent_software}

Use \lstinline|module avail| to check for centrally installed software.

The subsections below only cover a couple of installed software packages, more are available.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%
% GROMACS %
%%%%%%%%%%%

\subsection{GROMACS}
\label{sec:gpu_ugent_software_gromacs}

Please consult \lstinline|module avail GROMACS| for a list of installed versions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%
% Horovod %
%%%%%%%%%%%

\subsection{Horovod}
\label{sec:gpu_ugent_software_horovod}

Horovod can be used for (multi-node) multi-GPU TensorFlow/PyTorch calculations.

Please consult \lstinline|module avail Horovod| for a list of installed versions.

Horovod supports TensorFlow, Keras, PyTorch and MxNet (see \url{https://github.com/horovod/horovod#id9}), but should be run as an MPI application with \lstinline|mypmirun|.
(Horovod also provides it's own wrapper \lstinline|horovodrun|, not sure if it handles placement and others correctly).

At least for simple TensorFlow benchmarks, it looks like Horovod is a bit faster than usual autodetect multi-GPU TensorFlow without horovod, but it comes at the
cost of the code modifications to use horovod.

%TODO: use NCCL version (only check is to export NCCL_DEBUG=INFO)
%TODO: NCCL tuning https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html
%--> joltik: NCCL_NET_GDR_LEVEL=4 -> in environment



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%
% PyTorch %
%%%%%%%%%%%

\subsection{PyTorch}
\label{sec:gpu_ugent_software_pytorch}

Please consult \lstinline|module avail PyTorch| for a list of installed versions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%
% TensorFlow %
%%%%%%%%%%%%%%

\subsection{TensorFlow}
\label{sec:gpu_ugent_software_tensorflow}

Please consult \lstinline|module avail TensorFlow| for a list of installed versions.

\strong{Note: for running TensorFlow calculations on multiple GPUs and/or on more than one workernode,
use \lstinline|Horovod|, see section~\ref{sec:gpu_ugent_software_horovod}.}

\subsubsection{Example TensorFlow job script}
\label{sec:gpu_ugent_software_tensorflow_example_job_script}
\examplecode{bash}{TensorFlow_GPU_joltik.sh}

%TODO: add intel cpu params (only for cpu, but mention it anyway) and data format (channel_first  / NCHW)
% OMP_NUM_THREADS=$PBS_VARIABLE_FOR_CORES
% KMP_BLOCKTIME=0
% KMP_AFFINITY="granularity=fine,verbose,compact,1,0"
% KMP_SETTINGS=1

%TODO: quid tensocores and bfloat16 etc etc?

%TODO: even on single node, horovod is faster (5\%) with nccl? (at least for simple benchmark)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%
% Getting help %
%%%%%%%%%%%%%%%%

\section{Getting help}
\label{sec:gpu_ugent_help}

In case of questions or problems, please contact the \hpcTeam via \hpcinfo, and clearly indicate that your question
relates to the \lstinline|joltik| cluster by adding \lstinline|[joltik]| in the email subject.
